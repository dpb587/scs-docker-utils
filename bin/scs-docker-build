#!/usr/bin/env python

import argparse
import boto.ec2
import datetime
import hashlib
import os
import re
import simplejson
import string
import subprocess
import sys
import time
import urllib2
import yaml

cli = argparse.ArgumentParser(description='Utility for deploying a SCS image.')
cli.add_argument('environment_name', help='Environment name (e.g. prod)')
cli.add_argument('service_name', help='Service name (e.g. public-blog)')
cli.add_argument('role_name', help='Role name (e.g. web-wordpress)')
cli.add_argument('manifest', help='Local path to configuration file')
cli.add_argument('--source-reference', help='Commit reference to use when a manifest does not provide a canonical one', metavar='REF')
cli.add_argument('--cache-s3-bucket', help='An S3 bucket for caching built images', metavar='BUCKET')
cli.add_argument('--cache-s3-prefix', help='An optional prefix to use when caching built images to S3', metavar='PREFIX', default='')
cli.add_argument('--tmppath', help='A temporary path to work from when building images', metavar='PREFIX')
cli.add_argument('--verbose', '-v', action='count', help='Use multiple times to increase verbosity: none = quiet, 1 = completions, 2 = summaries, 3 = details')

cliargs = cli.parse_args()


#
# setup our basics
#

s3api = boto.connect_s3()

DEVNULL = open(os.devnull, 'w')

if cliargs.verbose > 2:
  TASK_STDOUT = sys.stdout
  TASK_STDERR = sys.stderr
else:
  TASK_STDOUT = DEVNULL
  TASK_STDERR = DEVNULL

if cliargs.cache_s3_bucket:
  s3cache = s3api.get_bucket(cliargs.cache_s3_bucket)
else:
  s3cache = False


#
# load configuration
#

config_runtime = yaml.safe_load(open(cliargs.manifest, 'r').read())


#
# resolve source reference
#

if 'git' == config_runtime['source']['type']:
  if 'reference' in config_runtime['source']:
    niceref = config_runtime['source']['reference']
  else:
    niceref = 'master'

  if not re.search(r"[a-f0-9]{40}", niceref):
    if not cliargs.source_reference:
      raise RuntimeError('Unable to resolve source reference "%s" and --source-reference was not specified.' % niceref)

    realref = cliargs.source_reference
  else:
    realref = niceref
else:
  raise RuntimeError('Unexpected source type.')


#
# identifiers
#

identCommon = '%s--%s--%s' % (
  cliargs.environment_name,
  cliargs.service_name,
  cliargs.role_name,
)
identCompilation = hashlib.md5(
  '%s\n%s\n%s\n%s' % (
    identCommon,
    realref,
    yaml.dump(config_runtime['source']),
    yaml.dump(config_runtime['config']),
  )
).hexdigest()
identFriendly = '%s--%s' % ( identCommon, identCompilation[:12] )

if cliargs.tmppath:
  tmppath = cliargs.tmppath
else:
  tmppath = '/tmp/%s' % identFriendly

#
# check for local copy
#

isLocal = False

if 0 == subprocess.call([ 'docker', 'inspect', '%s' % identFriendly ], stdout=DEVNULL, stderr=DEVNULL):
  isLocal = True


#
# check for remote copy
#

isRemote = False

if not isLocal and s3cache:
  cachekey = s3cache.get_key('%s%s.tar' % ( cliargs.cache_s3_prefix, identFriendly ))

  if None != cachekey:
    isRemote = True

    if cliargs.verbose > 1:
      sys.stdout.write('downloading cached image...\n')

    cachekey.get_contents_to_filename('/tmp/scs--%s.tar' % identFriendly)

    if cliargs.verbose > 0:
      sys.stdout.write('downloaded cached images\n')


    if cliargs.verbose > 1:
      sys.stdout.write('importing cached image...\n')

    subprocess.call([ 'docker', 'load' ], stdin=open('/tmp/scs--%s.tar' % identFriendly, 'r'), stdout=TASK_STDOUT, stderr=TASK_STDERR)

    if cliargs.verbose > 0:
      sys.stdout.write('imported cached image\n')


    os.remove('/tmp/scs--%s.tar' % identFriendly)

    isLocal = True


#
# rebuild the image
#

if not isLocal:
  #
  # get a local copy of the repo
  #

  if 'git' == config_runtime['source']['type']:
    if os.path.exists('%s/.git' % tmppath):
      os.chdir(tmppath)

      if subprocess.call([ 'git', 'diff', '--exit-code', '--quiet' ], stdout=TASK_STDOUT, stderr=TASK_STDERR):
        raise RuntimeError('The working directory "%s" is not clean.' % tmppath)

      subprocess.check_call([ 'git', 'fetch' ], stdout=TASK_STDOUT, stderr=TASK_STDERR)
    else:
      if cliargs.verbose > 1:
        sys.stdout.write('cloning repository...\n')

      subprocess.check_call([ 'git', 'clone', config_runtime['source']['url'], tmppath ], stdout=TASK_STDOUT, stderr=TASK_STDERR)

      if cliargs.verbose > 0:
        sys.stdout.write('cloned repository\n')

    os.chdir(tmppath)

    if cliargs.verbose > 1:
      sys.stdout.write('checking out reference...\n')

    subprocess.check_call([ 'git', 'checkout', realref ], stdout=TASK_STDOUT, stderr=TASK_STDERR)

    sys.stdout.write('checked out reference\n')
  else:
    raise RuntimeError('Unexpected source type')


  #
  # load scs manifest
  #

  if not os.path.exists('scs/manifest.yaml'):
    raise RuntimeError('Unable to find scs/manifest.yaml')

  config_definition = yaml.safe_load(open('scs/manifest.yaml', 'r').read())


  #
  # runtime basics
  #

  if not os.path.exists('scs/runtime'):
    os.mkdir('scs/runtime', 0700)

  open('scs/runtime/compilation', 'w').write(identCompilation)


  #
  # generate Dockerfile
  #

  dockerfile = []

  dockerfile.append('FROM %s' % config_definition['docker']['from'])
  dockerfile.append('RUN /bin/echo "deb http://archive.ubuntu.com/ubuntu/ precise universe" >> /etc/apt/sources.list && /usr/bin/apt-get update && /usr/bin/apt-get install -y wget ca-certificates && /usr/bin/wget https://apt.puppetlabs.com/puppetlabs-release-precise.deb && /usr/bin/dpkg -i puppetlabs-release-precise.deb && /bin/rm puppetlabs-release-precise.deb && /usr/bin/apt-get update && /usr/bin/apt-get install -y puppet && /usr/bin/puppet module install puppetlabs/stdlib && /usr/bin/apt-get clean && /bin/rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*')
  dockerfile.append('ADD . /scs')
  dockerfile.append('ENV SCS_ENVIRONMENT %s' % cliargs.environment_name)
  dockerfile.append('ENV SCS_SERVICE %s' % cliargs.service_name)
  dockerfile.append('ENV SCS_ROLE %s' % cliargs.role_name)
  dockerfile.append('RUN /usr/bin/puppet apply --modulepath=/scs/scs/puppet:/etc/puppet/modules:/usr/share/puppet/modules /scs/scs/runtime/puppet.pp')
  dockerfile.append('RUN /usr/bin/apt-get -y remove --purge puppet && /usr/bin/apt-get -y autoremove --purge && /usr/bin/apt-get clean && /bin/rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*')

  if 'volumes' in config_definition:
    for volume in config_definition['volumes']:
      dockerfile.append('VOLUME /scs/mnt/%s' % volume)

  if 'provides' in config_definition:
    for provision in config_definition['provides']:
      dockerfile.append('EXPOSE %s' % config_definition['provides'][provision]['port'])

  dockerfile.append('EXPOSE 9001')

  dockerfile.append('ENTRYPOINT [ "/scs/scs/bin/run" ]')

  open('Dockerfile', 'w').write('\n'.join(dockerfile))


  #
  # generate puppet
  #

  puppetfile = [
    '$SCS_ENVIRONMENT = "%s"' % cliargs.environment_name,
    '$SCS_SERVICE = "%s"' % cliargs.service_name,
    '$SCS_ROLE = "%s"' % cliargs.role_name,
  ]

  if not 'main' in config_runtime['config']:
    puppetfile.append('include scs')

  for parts in config_runtime['config']:
    if 'main' == parts:
      puppetfile.append('ensure_resource("class", "scs", parseyaml("%s"))' % yaml.dump(config_runtime['config']['main']))
    elif 'puppet' == parts:
      for ptype in config_runtime['config'][parts]:
        for pname in config_runtime['config'][parts][ptype]:
          puppetfile.append('ensure_resource("%s", "%s", parseyaml("%s"))' % ( ptype, pname, yaml.dump(config_runtime['config'][parts][ptype][pname]) ))
    else:
      for k in config_runtime['config'][parts]:
        puppetfile.append('ensure_resource("scs::%s", "%s", parseyaml("%s"))' % ( parts, k, yaml.dump(config_runtime['config'][parts][k]) ))

  open('scs/runtime/puppet.pp', 'w').write('\n'.join(puppetfile))


  #
  # build
  #

  if cliargs.verbose > 1:
    sys.stdout.write('building image...\n')

  subprocess.check_call(
    'docker build -rm -t "%s" .' % identFriendly,
    shell=True,
    stdout=TASK_STDOUT,
    stderr=TASK_STDERR,
  )

  if cliargs.verbose > 0:
    sys.stdout.write('built image\n')


#
# upload as necessary
#

if not isLocal and s3cache and not isRemote:
  if cliargs.verbose > 1:
    sys.stdout.write('exporting cache image...\n')

  subprocess.call(
    [ 'docker', 'save', identFriendly ],
    stdout=open('/tmp/scs--%s.tar' % identFriendly, 'w'),
    stderr=TASK_STDERR
  )

  if cliargs.verbose > 0:
    sys.stdout.write('exported cache image\n')


  cachekey = s3cache.new_key('%s%s.tar' % ( cliargs.cache_s3_prefix, identFriendly ))

  if cliargs.verbose > 1:
    sys.stdout.write('uploading cache image...\n')

  cachekey.set_contents_from_file(open('/tmp/scs--%s.tar' % identFriendly, 'r'))

  if cliargs.verbose > 0:
    sys.stdout.write('uploaded cache image\n')

  os.remove('/tmp/scs--%s.tar' % identFriendly)
