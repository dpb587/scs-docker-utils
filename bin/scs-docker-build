#!/usr/bin/env python

import argparse
import boto.ec2
import datetime
import hashlib
import os
import simplejson
import string
import subprocess
import sys
import time
import urllib2
import yaml

cli = argparse.ArgumentParser(description='Utility for deploying a SCS image.')
cli.add_argument('manifest', help='Local path to configuration file')
cli.add_argument('--cache-s3-bucket', help='An S3 bucket for caching built images', metavar='BUCKET')
cli.add_argument('--cache-s3-prefix', help='An optional prefix to use when caching built images to S3', metavar='PREFIX', default='')
cli.add_argument('--path', help='Use a specific working directory')
cli.add_argument('--verbose', '-v', action='count', help='Use multiple times to increase verbosity: none = quiet, 1 = completions, 2 = summaries, 3 = details')

cliargs = cli.parse_args()

if not cliargs.path:
  raise RuntimeError('--path must be specified')


#
# setup our basics
#

ec2instance = simplejson.loads(urllib2.urlopen('http://169.254.169.254/latest/dynamic/instance-identity/document').read())
ec2api = boto.ec2.connect_to_region(ec2instance['region'])
s3api = boto.connect_s3()

DEVNULL = open(os.devnull, 'w')

if cliargs.verbose > 2:
  TASK_STDOUT = sys.stdout
  TASK_STDERR = sys.stderr
else:
  TASK_STDOUT = DEVNULL
  TASK_STDERR = DEVNULL

if cliargs.cache_s3_bucket:
  s3cache = s3api.get_bucket(cliargs.cache_s3_bucket)
else:
  s3cache = False


#
# load configuration
#

config_runtime = yaml.safe_load(open(cliargs.manifest, 'r').read())


#
# identifiers
#

if 'git' == config_runtime['source']['type']:
  if 'reference' in config_runtime['source']:
    niceref = config_runtime['source']['reference']
  else:
    niceref = 'master'

  if 40 != len(niceref):
    p = subprocess.Popen(
      'git ls-remote "%s" | grep -E \'refs/(heads|tags)/%s\' | awk \'{ print $1 }\'' % ( config_runtime['source']['url'], niceref ),
      shell=True,
      stdout=subprocess.PIPE
    )

    if 0 < p.returncode:
      raise RuntimeError('Unable to resolve source reference.')

    realref = p.stdout.read().strip()
else:
  raise RuntimeError('Unexpected source type.')


nid = '%s--%s--%s' % (
  config_runtime['about']['environment'],
  config_runtime['about']['service'],
  config_runtime['about']['role'],
)
rid = hashlib.md5('%s\n%s\n%s\n%s' % ( nid, yaml.dump(config_runtime['about']), yaml.dump(config_runtime['source']), yaml.dump(config_runtime['config']) )).hexdigest()
uid = '%s--%s' % ( nid, rid[:12] )


#
# check for local copy
#

isLocal = False

if 0 == subprocess.call([ 'docker', 'inspect', '%s' % uid ], stdout=DEVNULL, stderr=DEVNULL):
  isLocal = True


#
# check for remote copy
#

isRemote = False

if not isLocal and s3cache:
  cachekey = s3cache.get_key('%s%s.tar' % ( cliargs.cache_s3_prefix, uid ))

  if None != cachekey:
    isRemote = True

    if cliargs.verbose > 1:
      sys.stdout.write('downloading cached image...\n')

    cachekey.get_contents_to_filename('/tmp/scs--%s.tar' % uid)

    if cliargs.verbose > 0:
      sys.stdout.write('downloaded cached images\n')


    if cliargs.verbose > 1:
      sys.stdout.write('importing cached image...\n')

    subprocess.call([ 'docker', 'load' ], stdin=open('/tmp/scs--%s.tar' % uid, 'r'), stdout=TASK_STDOUT, stderr=TASK_STDERR)

    if cliargs.verbose > 0:
      sys.stdout.write('imported cached image\n')


    os.remove('/tmp/scs--%s.tar' % uid)

    isLocal = True


#
# build when necessary
#

if not isLocal:
  if 'git' == config_runtime['source']['type']:
    #
    # cloning
    #

    if os.path.exists('%s/.git' % cliargs.path):
      os.chdir(cliargs.path)

      if subprocess.call([ 'git', 'diff', '--exit-code', '--quiet' ], stdout=TASK_STDOUT, stderr=TASK_STDERR):
        raise RuntimeError('The working directory "%s" is not clean.' % cliargs.path)
    else:
      if cliargs.verbose > 1:
        sys.stdout.write('cloning repository...\n')

      subprocess.check_call([ 'git', 'clone', config_runtime['source']['url'], cliargs.path ], stdout=TASK_STDOUT, stderr=TASK_STDERR)

      if cliargs.verbose > 0:
        sys.stdout.write('cloned repository\n')

    os.chdir(cliargs.path)

    if cliargs.verbose > 1:
      sys.stdout.write('checking out reference...\n')

    subprocess.check_call([ 'git', 'checkout', realref ], stdout=TASK_STDOUT, stderr=TASK_STDERR)

    sys.stdout.write('checked out reference\n')
  else:
    raise RuntimeError('Unexpected source type')


  #
  # discover scs manifest
  #

  if not os.path.exists('scs/manifest.yaml'):
    raise RuntimeError('Unable to find scs/manifest.yaml')

  config_definition = yaml.safe_load(open('scs/manifest.yaml', 'r').read())


  #
  # runtime basics
  #

  if not os.path.exists('scs/runtime'):
    os.mkdir('scs/runtime', 0700)

  open('scs/runtime/rid', 'w').write(rid)


  #
  # generate docker
  #

  dockerfile = []

  dockerfile.append('FROM %s' % config_definition['docker']['from'])
  dockerfile.append('RUN /bin/echo "deb http://archive.ubuntu.com/ubuntu/ precise universe" >> /etc/apt/sources.list && /usr/bin/apt-get update && /usr/bin/apt-get install -y wget ca-certificates && /usr/bin/wget https://apt.puppetlabs.com/puppetlabs-release-precise.deb && /usr/bin/dpkg -i puppetlabs-release-precise.deb && /bin/rm puppetlabs-release-precise.deb && /usr/bin/apt-get update && /usr/bin/apt-get install -y puppet && /usr/bin/puppet module install puppetlabs/stdlib && /usr/bin/apt-get clean && /bin/rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*')
  dockerfile.append('ADD . /scs')
  dockerfile.append('ENV SCS_ENVIRONMENT %s' % config_runtime['about']['environment'])
  dockerfile.append('ENV SCS_SERVICE %s' % config_runtime['about']['service'])
  dockerfile.append('ENV SCS_ROLE %s' % config_runtime['about']['role'])
  dockerfile.append('RUN /usr/bin/puppet apply --modulepath=/scs/scs/puppet:/etc/puppet/modules:/usr/share/puppet/modules /scs/scs/runtime/puppet.pp')
  dockerfile.append('RUN /usr/bin/apt-get -y remove --purge puppet && /usr/bin/apt-get clean && /bin/rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*')

  if 'volumes' in config_definition:
    for volume in config_definition['volumes']:
      dockerfile.append('VOLUME /scs/mnt/%s' % volume)

  if 'provides' in config_definition:
    for provision in config_definition['provides']:
      dockerfile.append('EXPOSE %s' % config_definition['provides'][provision]['port'])

  dockerfile.append('EXPOSE 9001')

  dockerfile.append('ENTRYPOINT [ "/scs/scs/bin/run" ]')

  open('Dockerfile', 'w').write('\n'.join(dockerfile))


  #
  # generate puppet
  #

  puppetfile = []

  if not 'main' in config_runtime['config']:
    puppetfile.append('include scs')

  for parts in config_runtime['config']:
    if 'main' == parts:
      puppetfile.append('ensure_resource("class", "scs", parseyaml("%s"))' % yaml.dump(config_runtime['config']['main']))
    else:
      for k in config_runtime['config'][parts]:
        puppetfile.append('ensure_resource("scs::%s", "%s", parseyaml("%s"))' % ( parts, k, yaml.dump(config_runtime['config'][parts][k]) ))

  open('scs/runtime/puppet.pp', 'w').write('\n'.join(puppetfile))


  #
  # build
  #

  if cliargs.verbose > 1:
    sys.stdout.write('building image...\n')

  subprocess.check_call(
    'docker build -rm -t "%s" .' % uid,
    shell=True,
    stdout=TASK_STDOUT,
    stderr=TASK_STDERR,
  )

  if cliargs.verbose > 0:
    sys.stdout.write('built image\n')


#
# upload as necessary
#

if not isLocal and s3cache and not isRemote:
  if cliargs.verbose > 1:
    sys.stdout.write('exporting cache image...\n')

  subprocess.call(
    [ 'docker', 'save', uid ],
    stdout=open('/tmp/scs--%s.tar' % uid, 'w'),
    stderr=TASK_STDERR
  )

  if cliargs.verbose > 0:
    sys.stdout.write('exported cache image\n')


  cachekey = s3cache.new_key('%s%s.tar' % ( cliargs.cache_s3_prefix, uid ))

  if cliargs.verbose > 1:
    sys.stdout.write('uploading cache image...\n')

  cachekey.set_contents_from_file(open('/tmp/scs--%s.tar' % uid, 'r'))

  if cliargs.verbose > 0:
    sys.stdout.write('uploaded cache image\n')

  os.remove('/tmp/scs--%s.tar' % uid)
